{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Song popularity dataset\n",
    "Predicting the popularity of a song"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing library's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import math\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.activations import linear, relu, softmax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "tf.autograph.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA/Pre-Processing\n",
    "Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_name</th>\n",
       "      <th>song_popularity</th>\n",
       "      <th>song_duration_ms</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>key</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>audio_mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>audio_valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Boulevard of Broken Dreams</td>\n",
       "      <td>73</td>\n",
       "      <td>262333</td>\n",
       "      <td>0.005520</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0589</td>\n",
       "      <td>-4.095</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>167.060</td>\n",
       "      <td>4</td>\n",
       "      <td>0.474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In The End</td>\n",
       "      <td>66</td>\n",
       "      <td>216933</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1080</td>\n",
       "      <td>-6.407</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0498</td>\n",
       "      <td>105.256</td>\n",
       "      <td>4</td>\n",
       "      <td>0.370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Seven Nation Army</td>\n",
       "      <td>76</td>\n",
       "      <td>231733</td>\n",
       "      <td>0.008170</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.447000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2550</td>\n",
       "      <td>-7.828</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0792</td>\n",
       "      <td>123.881</td>\n",
       "      <td>4</td>\n",
       "      <td>0.324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>By The Way</td>\n",
       "      <td>74</td>\n",
       "      <td>216933</td>\n",
       "      <td>0.026400</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.003550</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1020</td>\n",
       "      <td>-4.938</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1070</td>\n",
       "      <td>122.444</td>\n",
       "      <td>4</td>\n",
       "      <td>0.198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How You Remind Me</td>\n",
       "      <td>56</td>\n",
       "      <td>223826</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.1130</td>\n",
       "      <td>-5.065</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0313</td>\n",
       "      <td>172.011</td>\n",
       "      <td>4</td>\n",
       "      <td>0.574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    song_name  song_popularity  song_duration_ms  \\\n",
       "0  Boulevard of Broken Dreams               73            262333   \n",
       "1                  In The End               66            216933   \n",
       "2           Seven Nation Army               76            231733   \n",
       "3                  By The Way               74            216933   \n",
       "4           How You Remind Me               56            223826   \n",
       "\n",
       "   acousticness  danceability  energy  instrumentalness  key  liveness  \\\n",
       "0      0.005520         0.496   0.682          0.000029    8    0.0589   \n",
       "1      0.010300         0.542   0.853          0.000000    3    0.1080   \n",
       "2      0.008170         0.737   0.463          0.447000    0    0.2550   \n",
       "3      0.026400         0.451   0.970          0.003550    0    0.1020   \n",
       "4      0.000954         0.447   0.766          0.000000   10    0.1130   \n",
       "\n",
       "   loudness  audio_mode  speechiness    tempo  time_signature  audio_valence  \n",
       "0    -4.095           1       0.0294  167.060               4          0.474  \n",
       "1    -6.407           0       0.0498  105.256               4          0.370  \n",
       "2    -7.828           1       0.0792  123.881               4          0.324  \n",
       "3    -4.938           1       0.1070  122.444               4          0.198  \n",
       "4    -5.065           1       0.0313  172.011               4          0.574  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"archive/song_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18835, 15)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete duplicates, based on song name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13070, 15)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates(subset='song_name', keep='first', inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the composition of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 13070 entries, 0 to 18834\n",
      "Data columns (total 15 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   song_name         13070 non-null  object \n",
      " 1   song_popularity   13070 non-null  int64  \n",
      " 2   song_duration_ms  13070 non-null  int64  \n",
      " 3   acousticness      13070 non-null  float64\n",
      " 4   danceability      13070 non-null  float64\n",
      " 5   energy            13070 non-null  float64\n",
      " 6   instrumentalness  13070 non-null  float64\n",
      " 7   key               13070 non-null  int64  \n",
      " 8   liveness          13070 non-null  float64\n",
      " 9   loudness          13070 non-null  float64\n",
      " 10  audio_mode        13070 non-null  int64  \n",
      " 11  speechiness       13070 non-null  float64\n",
      " 12  tempo             13070 non-null  float64\n",
      " 13  time_signature    13070 non-null  int64  \n",
      " 14  audio_valence     13070 non-null  float64\n",
      "dtypes: float64(9), int64(5), object(1)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(15,10))\n",
    "#sns.heatmap(df.corr(),annot= True,)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4f69fbba53466c8d0a0ed7965dcad3",
       "version_major": 2,
       "version_minor": 0
      },
      "image/png": "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",
      "text/html": [
       "\n",
       "            <div style=\"display: inline-block;\">\n",
       "                <div class=\"jupyter-widgets widget-label\" style=\"text-align: center;\">\n",
       "                    Figure\n",
       "                </div>\n",
       "                <img src='data:image/png;base64,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' width=640.0/>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#df.hist(column=['energy', 'loudness'])\n",
    "plt.hist(df['energy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loudness and Energy is strong correlated, so we can drop one of them, because they give the same info to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('loudness', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13070, 14)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop(['song_name','song_popularity'],axis=1).to_numpy(), df['song_popularity'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13070, 12)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First model trained with all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_regression = Sequential(\n",
    "    [\n",
    "        #tf.keras.Input(shape=(14,)),  # specify input shape\n",
    "        Dense(10, activation='relu', name=\"L1\", kernel_regularizer=tf.keras.regularizers.l2(0.1)), # Regularization\n",
    "        Dense(5, activation='relu', name=\"L2\", kernel_regularizer=tf.keras.regularizers.l2(0.1)), # Regularization\n",
    "        Dense(1, activation='linear', name=\"Output\"),  # Linear activation for regression\n",
    "    ], name=\"my_regression_model\"\n",
    ")\n",
    "model_regression.compile(\n",
    "    loss='mean_squared_error', \n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history_regression = model_regression.fit(\n",
    "#     X, y,\n",
    "#     epochs=100\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second model, where dataset is divided in training data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (7842, 12) y_train.shape (7842,)\n",
      "X_test.shape (5228, 12) y_test.shape (5228,)\n"
     ]
    }
   ],
   "source": [
    "# using the train test split function \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y , test_size=0.4, random_state=1)\n",
    "print(\"X_train.shape\", X_train.shape, \"y_train.shape\", y_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape, \"y_test.shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_regression_60_40 = Sequential(\n",
    "    [\n",
    "\n",
    "        #tf.keras.Input(shape=(14,)),  # specify input shape\n",
    "        Dense(10, activation='relu', name=\"L1\", kernel_regularizer=tf.keras.regularizers.l2(0.1)), # Regularization\n",
    "        Dense(5, activation='relu', name=\"L2\", kernel_regularizer=tf.keras.regularizers.l2(0.1)), # Regularization\n",
    "        Dense(1, activation='linear', name=\"Output\"),  # Linear activation for regression\n",
    "\n",
    "    ], name=\"my_regression_model_60_40\"\n",
    ")\n",
    "\n",
    "model_regression_60_40.compile(\n",
    "    loss='mean_squared_error', \n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early_stop= EarlyStopping(patience=20)\n",
    "\n",
    "# history_regression_60_40 = model_regression_60_40.fit(\n",
    "#     X_train, y_train,\n",
    "#     epochs=200,\n",
    "#     callbacks=[early_stop]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(pd.DataFrame(model_regression_60_40.history.history))\n",
    "# plt.ylim(0, 1000)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number_of_tests = 200\n",
    "# sum_difference = 0\n",
    "\n",
    "# for i in range(number_of_tests):\n",
    "#     prediction = model_regression_60_40.predict(X_test[i].reshape(1,14))\n",
    "#     print(f\"{prediction} -> {y_test[i]}\")\n",
    "#     sum_difference += np.power(prediction - y_test[i],2)\n",
    "    \n",
    "# print(f\"MSE: {(sum_difference/number_of_tests)[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third model; tuned hyperparameters, 60% training, 40% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (7842, 12) y_train.shape (7842,)\n",
      "X_test.shape (5228, 12) y_test.shape (5228,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y , test_size=0.4, random_state=1)\n",
    "print(\"X_train.shape\", X_train.shape, \"y_train.shape\", y_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape, \"y_test.shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best till now: 400,300,200,100,50 Lambda=0.1 alpha=0.0001 => loss = 40\n",
    "#Best for unseen data: 400,200,50 Lambda=0.1 alpha=0.0001 => loss = 140\n",
    "model_regression_tuned = Sequential(\n",
    "    [\n",
    "\n",
    "        Dense(400, activation='relu', name=\"L1\", kernel_regularizer=tf.keras.regularizers.l2(0.1)), # CHANGED: Regularization\n",
    "        Dense(200, activation='relu', name=\"L2\", kernel_regularizer=tf.keras.regularizers.l2(0.1)), # CHANGED: Regularization\n",
    "        Dense(100, activation='relu', name=\"L4\", kernel_regularizer=tf.keras.regularizers.l2(0.1)),\n",
    "        Dense(50, activation='relu', name=\"L3\", kernel_regularizer=tf.keras.regularizers.l2(0.1)), # CHANGED: Regularization\n",
    "        Dense(1, activation='relu', name=\"Output\"),  # Linear activation for regression\n",
    "\n",
    "    ], name=\"my_regression_model_tuned\"\n",
    ")\n",
    "\n",
    "model_regression_tuned.compile(\n",
    "    loss='mean_squared_error', \n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), # CHANGED: alpha\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "197/197 [==============================] - 2s 5ms/step - loss: 1916.6332 - val_loss: 624.8534\n",
      "Epoch 2/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 546.0234 - val_loss: 510.1134\n",
      "Epoch 3/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 502.7149 - val_loss: 490.0831\n",
      "Epoch 4/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 489.3792 - val_loss: 480.4281\n",
      "Epoch 5/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 480.3439 - val_loss: 475.0015\n",
      "Epoch 6/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 474.7603 - val_loss: 466.2848\n",
      "Epoch 7/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 467.9045 - val_loss: 460.2832\n",
      "Epoch 8/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 461.7546 - val_loss: 456.8422\n",
      "Epoch 9/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 456.7812 - val_loss: 453.3173\n",
      "Epoch 10/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 452.8998 - val_loss: 449.1318\n",
      "Epoch 11/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 448.9331 - val_loss: 445.1836\n",
      "Epoch 12/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 445.4087 - val_loss: 442.4924\n",
      "Epoch 13/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 443.0470 - val_loss: 444.1134\n",
      "Epoch 14/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 439.8323 - val_loss: 437.8305\n",
      "Epoch 15/200\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 437.4787 - val_loss: 435.3697\n",
      "Epoch 16/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 434.1801 - val_loss: 436.7554\n",
      "Epoch 17/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 433.1036 - val_loss: 435.8261\n",
      "Epoch 18/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 429.8341 - val_loss: 434.4628\n",
      "Epoch 19/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 427.4377 - val_loss: 428.5940\n",
      "Epoch 20/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 425.7057 - val_loss: 428.0379\n",
      "Epoch 21/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 424.6844 - val_loss: 427.5538\n",
      "Epoch 22/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 422.2383 - val_loss: 428.8460\n",
      "Epoch 23/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 422.0935 - val_loss: 425.4234\n",
      "Epoch 24/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 419.7361 - val_loss: 425.4012\n",
      "Epoch 25/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 418.4231 - val_loss: 425.2557\n",
      "Epoch 26/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 416.8189 - val_loss: 423.4378\n",
      "Epoch 27/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 415.1945 - val_loss: 422.1367\n",
      "Epoch 28/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 415.1465 - val_loss: 424.4711\n",
      "Epoch 29/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 413.1489 - val_loss: 420.9637\n",
      "Epoch 30/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 412.5685 - val_loss: 424.6107\n",
      "Epoch 31/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 410.6573 - val_loss: 422.8418\n",
      "Epoch 32/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 409.8610 - val_loss: 418.2421\n",
      "Epoch 33/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 408.3305 - val_loss: 417.4205\n",
      "Epoch 34/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 408.7443 - val_loss: 419.3404\n",
      "Epoch 35/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 407.5912 - val_loss: 417.1777\n",
      "Epoch 36/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 405.3662 - val_loss: 417.3074\n",
      "Epoch 37/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 404.9072 - val_loss: 417.4774\n",
      "Epoch 38/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 404.0619 - val_loss: 422.1679\n",
      "Epoch 39/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 404.6398 - val_loss: 414.9648\n",
      "Epoch 40/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 403.0831 - val_loss: 418.0949\n",
      "Epoch 41/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 403.2159 - val_loss: 414.3552\n",
      "Epoch 42/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 400.5894 - val_loss: 414.6517\n",
      "Epoch 43/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 400.5344 - val_loss: 413.7622\n",
      "Epoch 44/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 399.6122 - val_loss: 414.8211\n",
      "Epoch 45/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 398.3716 - val_loss: 414.7201\n",
      "Epoch 46/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 397.7711 - val_loss: 415.0787\n",
      "Epoch 47/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 397.4437 - val_loss: 415.3929\n",
      "Epoch 48/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 396.6717 - val_loss: 414.5192\n",
      "Epoch 49/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 395.6635 - val_loss: 414.7080\n",
      "Epoch 50/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 395.4039 - val_loss: 416.2692\n",
      "Epoch 51/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 395.8129 - val_loss: 414.8180\n",
      "Epoch 52/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 394.1428 - val_loss: 413.6449\n",
      "Epoch 53/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 393.7468 - val_loss: 411.9426\n",
      "Epoch 54/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 392.1174 - val_loss: 416.8866\n",
      "Epoch 55/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 390.7614 - val_loss: 412.5978\n",
      "Epoch 56/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 391.1830 - val_loss: 412.8568\n",
      "Epoch 57/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 391.3101 - val_loss: 416.3755\n",
      "Epoch 58/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 389.1304 - val_loss: 413.7979\n",
      "Epoch 59/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 391.6237 - val_loss: 412.9384\n",
      "Epoch 60/200\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 388.7186 - val_loss: 412.1499\n",
      "Epoch 61/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 386.8690 - val_loss: 418.4011\n",
      "Epoch 62/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 388.1211 - val_loss: 413.1883\n",
      "Epoch 63/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 386.1618 - val_loss: 412.5748\n",
      "Epoch 64/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 386.0870 - val_loss: 411.3045\n",
      "Epoch 65/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 384.5385 - val_loss: 414.0783\n",
      "Epoch 66/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 384.2024 - val_loss: 413.5197\n",
      "Epoch 67/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 384.4148 - val_loss: 411.8540\n",
      "Epoch 68/200\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 383.9151 - val_loss: 412.4129\n",
      "Epoch 69/200\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 382.2231 - val_loss: 414.5172\n",
      "Epoch 70/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 380.6656 - val_loss: 413.7363\n",
      "Epoch 71/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 380.2946 - val_loss: 413.5257\n",
      "Epoch 72/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 379.2979 - val_loss: 414.9255\n",
      "Epoch 73/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 378.8278 - val_loss: 412.3732\n",
      "Epoch 74/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 377.9613 - val_loss: 413.2968\n",
      "Epoch 75/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 378.3009 - val_loss: 412.6959\n",
      "Epoch 76/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 376.9202 - val_loss: 413.4521\n",
      "Epoch 77/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 375.1141 - val_loss: 412.0425\n",
      "Epoch 78/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 376.0923 - val_loss: 414.4503\n",
      "Epoch 79/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 375.4346 - val_loss: 411.6262\n",
      "Epoch 80/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 372.8450 - val_loss: 411.5045\n",
      "Epoch 81/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 372.2887 - val_loss: 412.6659\n",
      "Epoch 82/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 372.6132 - val_loss: 416.7476\n",
      "Epoch 83/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 371.3625 - val_loss: 413.8044\n",
      "Epoch 84/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 370.2291 - val_loss: 415.5132\n",
      "Epoch 85/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 368.1688 - val_loss: 424.2888\n",
      "Epoch 86/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 368.2807 - val_loss: 413.6610\n",
      "Epoch 87/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 367.0536 - val_loss: 418.2410\n",
      "Epoch 88/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 365.8563 - val_loss: 418.5833\n",
      "Epoch 89/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 365.4858 - val_loss: 416.5304\n",
      "Epoch 90/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 364.2668 - val_loss: 417.3505\n",
      "Epoch 91/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 364.0081 - val_loss: 416.5705\n",
      "Epoch 92/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 365.2594 - val_loss: 417.9624\n",
      "Epoch 93/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 364.2267 - val_loss: 415.4473\n",
      "Epoch 94/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 363.8710 - val_loss: 417.1677\n",
      "Epoch 95/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 361.2489 - val_loss: 421.8988\n",
      "Epoch 96/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 362.1246 - val_loss: 422.4563\n",
      "Epoch 97/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 358.3471 - val_loss: 423.4088\n",
      "Epoch 98/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 358.8846 - val_loss: 417.7576\n",
      "Epoch 99/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 357.1149 - val_loss: 415.8673\n",
      "Epoch 100/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 356.2773 - val_loss: 418.1277\n",
      "Epoch 101/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 357.2786 - val_loss: 421.1637\n",
      "Epoch 102/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 353.2155 - val_loss: 418.7379\n",
      "Epoch 103/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 354.0448 - val_loss: 420.8774\n",
      "Epoch 104/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 351.9404 - val_loss: 437.9445\n",
      "Epoch 105/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 352.0740 - val_loss: 421.0000\n",
      "Epoch 106/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 350.5837 - val_loss: 420.2867\n",
      "Epoch 107/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 350.7864 - val_loss: 422.9893\n",
      "Epoch 108/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 351.3105 - val_loss: 423.6206\n",
      "Epoch 109/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 346.5857 - val_loss: 427.6010\n",
      "Epoch 110/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 348.3683 - val_loss: 428.5361\n",
      "Epoch 111/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 347.3470 - val_loss: 430.7625\n",
      "Epoch 112/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 344.9943 - val_loss: 425.3410\n",
      "Epoch 113/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 342.9251 - val_loss: 423.4834\n",
      "Epoch 114/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 343.9217 - val_loss: 430.2551\n",
      "Epoch 115/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 341.6381 - val_loss: 422.8903\n",
      "Epoch 116/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 341.4541 - val_loss: 429.0601\n",
      "Epoch 117/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 338.5417 - val_loss: 437.3553\n",
      "Epoch 118/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 339.9260 - val_loss: 439.4416\n",
      "Epoch 119/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 338.2444 - val_loss: 434.5158\n",
      "Epoch 120/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 335.7690 - val_loss: 430.2277\n",
      "Epoch 121/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 340.5905 - val_loss: 435.9156\n",
      "Epoch 122/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 335.8911 - val_loss: 432.3471\n",
      "Epoch 123/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 334.1621 - val_loss: 428.7817\n",
      "Epoch 124/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 331.2643 - val_loss: 435.6276\n",
      "Epoch 125/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 334.9951 - val_loss: 432.1265\n",
      "Epoch 126/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 328.4202 - val_loss: 451.3950\n",
      "Epoch 127/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 334.9561 - val_loss: 435.5407\n",
      "Epoch 128/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 329.2616 - val_loss: 435.8782\n",
      "Epoch 129/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 327.7746 - val_loss: 435.0221\n",
      "Epoch 130/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 326.0199 - val_loss: 440.4380\n",
      "Epoch 131/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 326.3136 - val_loss: 443.4671\n",
      "Epoch 132/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 324.2185 - val_loss: 436.0576\n",
      "Epoch 133/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 323.2364 - val_loss: 440.1997\n",
      "Epoch 134/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 322.7776 - val_loss: 440.5732\n",
      "Epoch 135/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 321.2632 - val_loss: 437.3710\n",
      "Epoch 136/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 321.7515 - val_loss: 441.0988\n",
      "Epoch 137/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 318.5171 - val_loss: 442.6905\n",
      "Epoch 138/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 318.5249 - val_loss: 453.5979\n",
      "Epoch 139/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 316.9203 - val_loss: 443.7078\n",
      "Epoch 140/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 316.7392 - val_loss: 452.6304\n",
      "Epoch 141/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 316.0911 - val_loss: 451.2686\n",
      "Epoch 142/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 315.2008 - val_loss: 447.7206\n",
      "Epoch 143/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 311.3007 - val_loss: 454.3544\n",
      "Epoch 144/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 311.7737 - val_loss: 466.2699\n",
      "Epoch 145/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 309.4226 - val_loss: 448.0985\n",
      "Epoch 146/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 309.1320 - val_loss: 459.4696\n",
      "Epoch 147/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 308.5145 - val_loss: 457.9187\n",
      "Epoch 148/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 308.0956 - val_loss: 454.2624\n",
      "Epoch 149/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 304.4217 - val_loss: 456.6840\n",
      "Epoch 150/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 305.3800 - val_loss: 478.2729\n",
      "Epoch 151/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 313.3223 - val_loss: 456.9131\n",
      "Epoch 152/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 303.6490 - val_loss: 458.4977\n",
      "Epoch 153/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 302.4799 - val_loss: 460.6574\n",
      "Epoch 154/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 299.5983 - val_loss: 458.1437\n",
      "Epoch 155/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 300.4357 - val_loss: 469.6714\n",
      "Epoch 156/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 301.1505 - val_loss: 457.5016\n",
      "Epoch 157/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 299.0215 - val_loss: 462.7363\n",
      "Epoch 158/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 294.7627 - val_loss: 491.6079\n",
      "Epoch 159/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 297.5010 - val_loss: 460.2106\n",
      "Epoch 160/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 294.9515 - val_loss: 464.6168\n",
      "Epoch 161/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 296.9994 - val_loss: 464.1774\n",
      "Epoch 162/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 294.3513 - val_loss: 485.6433\n",
      "Epoch 163/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 299.1437 - val_loss: 464.9572\n",
      "Epoch 164/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 289.5615 - val_loss: 463.6997\n",
      "Epoch 165/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 288.0717 - val_loss: 470.9845\n",
      "Epoch 166/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 291.6650 - val_loss: 465.7818\n",
      "Epoch 167/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 288.9311 - val_loss: 477.8652\n",
      "Epoch 168/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 286.8839 - val_loss: 466.3569\n",
      "Epoch 169/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 285.9637 - val_loss: 472.0493\n",
      "Epoch 170/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 283.1119 - val_loss: 475.1323\n",
      "Epoch 171/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 285.0482 - val_loss: 494.3925\n",
      "Epoch 172/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 283.6561 - val_loss: 491.2289\n",
      "Epoch 173/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 283.7419 - val_loss: 475.3902\n",
      "Epoch 174/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 279.9735 - val_loss: 479.2108\n",
      "Epoch 175/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 279.1291 - val_loss: 476.3036\n",
      "Epoch 176/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 277.0086 - val_loss: 489.0217\n",
      "Epoch 177/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 278.8066 - val_loss: 480.2321\n",
      "Epoch 178/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 276.0588 - val_loss: 494.6740\n",
      "Epoch 179/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 279.0683 - val_loss: 483.4138\n",
      "Epoch 180/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 271.6903 - val_loss: 492.0810\n",
      "Epoch 181/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 274.3564 - val_loss: 492.3837\n",
      "Epoch 182/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 274.9688 - val_loss: 494.7609\n",
      "Epoch 183/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 270.3697 - val_loss: 492.2819\n",
      "Epoch 184/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 273.9372 - val_loss: 490.5561\n",
      "Epoch 185/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 274.1751 - val_loss: 494.1626\n",
      "Epoch 186/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 269.0765 - val_loss: 489.7267\n",
      "Epoch 187/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 270.8512 - val_loss: 492.7567\n",
      "Epoch 188/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 266.1807 - val_loss: 497.3777\n",
      "Epoch 189/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 266.9559 - val_loss: 515.5431\n",
      "Epoch 190/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 265.6897 - val_loss: 498.8719\n",
      "Epoch 191/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 265.4039 - val_loss: 494.1423\n",
      "Epoch 192/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 261.0344 - val_loss: 491.8408\n",
      "Epoch 193/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 259.3692 - val_loss: 511.4841\n",
      "Epoch 194/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 261.9459 - val_loss: 501.4132\n",
      "Epoch 195/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 260.4057 - val_loss: 497.9879\n",
      "Epoch 196/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 257.2164 - val_loss: 499.2920\n",
      "Epoch 197/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 257.1983 - val_loss: 503.6121\n",
      "Epoch 198/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 265.6132 - val_loss: 516.4385\n",
      "Epoch 199/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 258.4193 - val_loss: 524.8695\n",
      "Epoch 200/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 263.8109 - val_loss: 503.3742\n",
      "Epoch 1/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 319.5346 - val_loss: 283.8809\n",
      "Epoch 2/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 307.5048 - val_loss: 280.2978\n",
      "Epoch 3/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 304.5443 - val_loss: 282.5593\n",
      "Epoch 4/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 301.1937 - val_loss: 281.2925\n",
      "Epoch 5/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 303.6630 - val_loss: 288.9056\n",
      "Epoch 6/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 298.8495 - val_loss: 298.2664\n",
      "Epoch 7/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 299.0373 - val_loss: 289.3194\n",
      "Epoch 8/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 290.8310 - val_loss: 300.1593\n",
      "Epoch 9/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 289.9353 - val_loss: 299.4775\n",
      "Epoch 10/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 291.3815 - val_loss: 296.2437\n",
      "Epoch 11/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 286.7477 - val_loss: 310.6878\n",
      "Epoch 12/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 290.2214 - val_loss: 304.1653\n",
      "Epoch 13/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 286.6839 - val_loss: 319.5905\n",
      "Epoch 14/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 283.0451 - val_loss: 305.4422\n",
      "Epoch 15/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 280.6235 - val_loss: 307.8043\n",
      "Epoch 16/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 280.1106 - val_loss: 314.4833\n",
      "Epoch 17/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 280.2354 - val_loss: 310.5779\n",
      "Epoch 18/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 281.5396 - val_loss: 318.6095\n",
      "Epoch 19/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 280.3730 - val_loss: 309.0751\n",
      "Epoch 20/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 274.4501 - val_loss: 323.9122\n",
      "Epoch 21/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 272.9125 - val_loss: 319.4297\n",
      "Epoch 22/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 275.4593 - val_loss: 325.4588\n",
      "Epoch 23/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 273.0915 - val_loss: 316.1905\n",
      "Epoch 24/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 279.3132 - val_loss: 328.0187\n",
      "Epoch 25/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 266.7791 - val_loss: 336.3780\n",
      "Epoch 26/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 269.0366 - val_loss: 327.3849\n",
      "Epoch 27/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 272.5490 - val_loss: 327.3474\n",
      "Epoch 28/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 268.8584 - val_loss: 346.4852\n",
      "Epoch 29/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 268.8893 - val_loss: 344.8222\n",
      "Epoch 30/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 268.0713 - val_loss: 331.7036\n",
      "Epoch 31/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 260.6306 - val_loss: 338.1343\n",
      "Epoch 32/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 263.9496 - val_loss: 338.9923\n",
      "Epoch 33/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 258.3027 - val_loss: 337.0066\n",
      "Epoch 34/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 259.6862 - val_loss: 348.2602\n",
      "Epoch 35/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 264.9658 - val_loss: 338.3610\n",
      "Epoch 36/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 255.7242 - val_loss: 348.5831\n",
      "Epoch 37/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 256.0903 - val_loss: 334.9257\n",
      "Epoch 38/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 250.6117 - val_loss: 341.0559\n",
      "Epoch 39/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 253.4230 - val_loss: 340.3136\n",
      "Epoch 40/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 255.9281 - val_loss: 346.3461\n",
      "Epoch 41/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 249.8872 - val_loss: 357.6671\n",
      "Epoch 42/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 254.9555 - val_loss: 368.2461\n",
      "Epoch 43/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 248.5195 - val_loss: 345.3921\n",
      "Epoch 44/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 250.7543 - val_loss: 367.8206\n",
      "Epoch 45/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 250.7068 - val_loss: 349.5760\n",
      "Epoch 46/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 243.2756 - val_loss: 359.1461\n",
      "Epoch 47/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 245.9552 - val_loss: 351.4840\n",
      "Epoch 48/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 250.1064 - val_loss: 361.0822\n",
      "Epoch 49/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 242.7175 - val_loss: 349.3840\n",
      "Epoch 50/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 241.5149 - val_loss: 363.6212\n",
      "Epoch 51/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 242.8383 - val_loss: 366.0983\n",
      "Epoch 52/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 242.0017 - val_loss: 361.2215\n",
      "Epoch 53/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 240.3552 - val_loss: 357.8470\n",
      "Epoch 54/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 239.5692 - val_loss: 350.7968\n",
      "Epoch 55/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 237.3841 - val_loss: 363.7365\n",
      "Epoch 56/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 235.3706 - val_loss: 356.0407\n",
      "Epoch 57/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 232.5737 - val_loss: 362.6297\n",
      "Epoch 58/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 236.2926 - val_loss: 355.6860\n",
      "Epoch 59/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 233.3584 - val_loss: 364.5151\n",
      "Epoch 60/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 229.3205 - val_loss: 375.8579\n",
      "Epoch 61/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 233.6423 - val_loss: 372.1335\n",
      "Epoch 62/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 230.5756 - val_loss: 369.9123\n",
      "Epoch 63/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 227.9025 - val_loss: 384.4741\n",
      "Epoch 64/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 227.6575 - val_loss: 380.8665\n",
      "Epoch 65/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 227.8254 - val_loss: 389.8004\n",
      "Epoch 66/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 233.9289 - val_loss: 385.4977\n",
      "Epoch 67/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 233.2373 - val_loss: 372.4990\n",
      "Epoch 68/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 224.5111 - val_loss: 373.4779\n",
      "Epoch 69/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 224.4234 - val_loss: 373.8415\n",
      "Epoch 70/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 222.2369 - val_loss: 396.2204\n",
      "Epoch 71/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 221.6205 - val_loss: 372.5252\n",
      "Epoch 72/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 219.5253 - val_loss: 403.6032\n",
      "Epoch 73/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 233.3673 - val_loss: 392.7849\n",
      "Epoch 74/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 220.8213 - val_loss: 376.7623\n",
      "Epoch 75/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 224.5754 - val_loss: 386.3035\n",
      "Epoch 76/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 222.3787 - val_loss: 388.0842\n",
      "Epoch 77/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 217.7232 - val_loss: 388.0182\n",
      "Epoch 78/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 216.1443 - val_loss: 398.3936\n",
      "Epoch 79/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 240.0518 - val_loss: 383.4225\n",
      "Epoch 80/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 213.3390 - val_loss: 383.6090\n",
      "Epoch 81/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 211.7017 - val_loss: 392.4527\n",
      "Epoch 82/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 210.8125 - val_loss: 395.9020\n",
      "Epoch 83/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 209.8542 - val_loss: 392.0211\n",
      "Epoch 84/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 211.5701 - val_loss: 379.4969\n",
      "Epoch 85/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 215.6587 - val_loss: 406.6440\n",
      "Epoch 86/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 214.5550 - val_loss: 397.8917\n",
      "Epoch 87/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 210.9691 - val_loss: 391.4576\n",
      "Epoch 88/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 208.9568 - val_loss: 401.8353\n",
      "Epoch 89/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 213.5324 - val_loss: 410.2050\n",
      "Epoch 90/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 207.1074 - val_loss: 391.6259\n",
      "Epoch 91/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 206.9052 - val_loss: 399.7675\n",
      "Epoch 92/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 202.7860 - val_loss: 500.8312\n",
      "Epoch 93/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 220.0242 - val_loss: 393.7194\n",
      "Epoch 94/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 209.1327 - val_loss: 394.2465\n",
      "Epoch 95/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 203.0937 - val_loss: 400.7136\n",
      "Epoch 96/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 205.0000 - val_loss: 392.0697\n",
      "Epoch 97/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 199.2262 - val_loss: 415.4098\n",
      "Epoch 98/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 200.4197 - val_loss: 408.9434\n",
      "Epoch 99/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 200.5779 - val_loss: 417.7199\n",
      "Epoch 100/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 198.9294 - val_loss: 407.2440\n",
      "Epoch 101/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 199.3246 - val_loss: 398.4029\n",
      "Epoch 102/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 199.2187 - val_loss: 428.6893\n",
      "Epoch 103/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 209.4001 - val_loss: 411.5369\n",
      "Epoch 104/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 197.8635 - val_loss: 400.4457\n",
      "Epoch 105/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 196.5476 - val_loss: 409.8899\n",
      "Epoch 106/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 195.1927 - val_loss: 408.6263\n",
      "Epoch 107/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 195.0976 - val_loss: 418.3799\n",
      "Epoch 108/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 194.6503 - val_loss: 408.3720\n",
      "Epoch 109/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 192.4323 - val_loss: 409.8924\n",
      "Epoch 110/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 189.2352 - val_loss: 413.8204\n",
      "Epoch 111/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 194.6466 - val_loss: 438.7808\n",
      "Epoch 112/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 193.8044 - val_loss: 410.8321\n",
      "Epoch 113/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 190.4058 - val_loss: 414.6162\n",
      "Epoch 114/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 192.5363 - val_loss: 428.4063\n",
      "Epoch 115/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 191.2478 - val_loss: 410.0479\n",
      "Epoch 116/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 193.3221 - val_loss: 472.7501\n",
      "Epoch 117/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 216.7504 - val_loss: 414.0609\n",
      "Epoch 118/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 185.6700 - val_loss: 424.5224\n",
      "Epoch 119/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 192.0088 - val_loss: 415.6665\n",
      "Epoch 120/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 184.6317 - val_loss: 428.6676\n",
      "Epoch 121/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 181.0046 - val_loss: 447.6785\n",
      "Epoch 122/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 193.4211 - val_loss: 416.5751\n",
      "Epoch 123/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 184.0010 - val_loss: 417.9245\n",
      "Epoch 124/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 183.1253 - val_loss: 420.7496\n",
      "Epoch 125/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 185.6129 - val_loss: 433.0912\n",
      "Epoch 126/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 182.8825 - val_loss: 431.8074\n",
      "Epoch 127/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 186.4968 - val_loss: 427.0426\n",
      "Epoch 128/200\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 177.0686 - val_loss: 446.1376\n",
      "Epoch 129/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 182.3234 - val_loss: 423.3038\n",
      "Epoch 130/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 178.3138 - val_loss: 445.3715\n",
      "Epoch 131/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 177.7268 - val_loss: 437.8786\n",
      "Epoch 132/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 175.3222 - val_loss: 442.3730\n",
      "Epoch 133/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 181.6393 - val_loss: 422.5785\n",
      "Epoch 134/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 179.4040 - val_loss: 427.3643\n",
      "Epoch 135/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 185.5202 - val_loss: 427.4164\n",
      "Epoch 136/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 188.6124 - val_loss: 448.4251\n",
      "Epoch 137/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 177.2899 - val_loss: 439.2961\n",
      "Epoch 138/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 178.2802 - val_loss: 456.4480\n",
      "Epoch 139/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 180.4401 - val_loss: 425.1810\n",
      "Epoch 140/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 181.4362 - val_loss: 428.8892\n",
      "Epoch 141/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 171.1510 - val_loss: 442.4220\n",
      "Epoch 142/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 177.8657 - val_loss: 440.8915\n",
      "Epoch 143/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 172.7430 - val_loss: 435.4440\n",
      "Epoch 144/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 171.5560 - val_loss: 445.7646\n",
      "Epoch 145/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 176.7666 - val_loss: 434.3209\n",
      "Epoch 146/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 167.3251 - val_loss: 439.6017\n",
      "Epoch 147/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 175.6144 - val_loss: 459.7939\n",
      "Epoch 148/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 181.0283 - val_loss: 429.8425\n",
      "Epoch 149/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 163.7863 - val_loss: 436.1082\n",
      "Epoch 150/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 167.5247 - val_loss: 453.5916\n",
      "Epoch 151/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 187.0206 - val_loss: 433.4174\n",
      "Epoch 152/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 170.8579 - val_loss: 434.3586\n",
      "Epoch 153/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 165.9069 - val_loss: 443.4677\n",
      "Epoch 154/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 166.4071 - val_loss: 444.2115\n",
      "Epoch 155/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 171.5791 - val_loss: 453.6535\n",
      "Epoch 156/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 164.5606 - val_loss: 450.2367\n",
      "Epoch 157/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 167.5507 - val_loss: 446.0738\n",
      "Epoch 158/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 163.2475 - val_loss: 467.3103\n",
      "Epoch 159/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 161.5014 - val_loss: 452.5978\n",
      "Epoch 160/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 162.9571 - val_loss: 463.2086\n",
      "Epoch 161/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 162.9858 - val_loss: 476.3152\n",
      "Epoch 162/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 164.4548 - val_loss: 446.6640\n",
      "Epoch 163/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 161.6182 - val_loss: 447.1378\n",
      "Epoch 164/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 170.5610 - val_loss: 460.3812\n",
      "Epoch 165/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 169.3479 - val_loss: 470.6254\n",
      "Epoch 166/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 161.5037 - val_loss: 446.6700\n",
      "Epoch 167/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 155.4185 - val_loss: 451.9705\n",
      "Epoch 168/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 156.7920 - val_loss: 458.3911\n",
      "Epoch 169/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 159.1296 - val_loss: 462.3195\n",
      "Epoch 170/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 158.7278 - val_loss: 469.7346\n",
      "Epoch 171/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 157.8333 - val_loss: 448.1652\n",
      "Epoch 172/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 157.1405 - val_loss: 453.0585\n",
      "Epoch 173/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 158.3879 - val_loss: 485.7585\n",
      "Epoch 174/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 158.1900 - val_loss: 492.9581\n",
      "Epoch 175/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 177.8835 - val_loss: 451.8447\n",
      "Epoch 176/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 152.5313 - val_loss: 488.7943\n",
      "Epoch 177/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 163.1682 - val_loss: 480.8130\n",
      "Epoch 178/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 154.8482 - val_loss: 481.3958\n",
      "Epoch 179/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 155.3895 - val_loss: 454.5464\n",
      "Epoch 180/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 150.4399 - val_loss: 478.2734\n",
      "Epoch 181/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 152.9020 - val_loss: 467.0482\n",
      "Epoch 182/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 155.4691 - val_loss: 469.1361\n",
      "Epoch 183/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 156.3193 - val_loss: 471.9027\n",
      "Epoch 184/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 164.9433 - val_loss: 455.7135\n",
      "Epoch 185/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 152.6664 - val_loss: 478.3878\n",
      "Epoch 186/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 156.7492 - val_loss: 472.7244\n",
      "Epoch 187/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 154.8547 - val_loss: 468.2747\n",
      "Epoch 188/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 150.3973 - val_loss: 462.4465\n",
      "Epoch 189/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 148.6888 - val_loss: 478.2697\n",
      "Epoch 190/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 150.0736 - val_loss: 480.2885\n",
      "Epoch 191/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 152.2179 - val_loss: 467.3908\n",
      "Epoch 192/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 144.3011 - val_loss: 476.3971\n",
      "Epoch 193/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 146.9124 - val_loss: 497.3770\n",
      "Epoch 194/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 147.3179 - val_loss: 499.9943\n",
      "Epoch 195/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 178.0595 - val_loss: 480.9802\n",
      "Epoch 196/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 151.0561 - val_loss: 470.0515\n",
      "Epoch 197/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 147.2049 - val_loss: 479.2303\n",
      "Epoch 198/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 143.6995 - val_loss: 488.1008\n",
      "Epoch 199/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 159.7815 - val_loss: 465.9147\n",
      "Epoch 200/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 145.7554 - val_loss: 466.7315\n",
      "Epoch 1/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 247.7920 - val_loss: 158.9800\n",
      "Epoch 2/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 231.2363 - val_loss: 187.3136\n",
      "Epoch 3/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 229.4501 - val_loss: 161.7762\n",
      "Epoch 4/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 225.4432 - val_loss: 170.5896\n",
      "Epoch 5/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 212.6633 - val_loss: 176.6075\n",
      "Epoch 6/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 213.4944 - val_loss: 160.7695\n",
      "Epoch 7/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 215.9307 - val_loss: 157.3139\n",
      "Epoch 8/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 205.4991 - val_loss: 166.4052\n",
      "Epoch 9/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 205.1368 - val_loss: 173.7186\n",
      "Epoch 10/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 210.5418 - val_loss: 175.0445\n",
      "Epoch 11/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 196.8958 - val_loss: 187.9165\n",
      "Epoch 12/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 200.2989 - val_loss: 172.3267\n",
      "Epoch 13/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 196.7963 - val_loss: 199.8626\n",
      "Epoch 14/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 201.6899 - val_loss: 304.9612\n",
      "Epoch 15/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 219.6263 - val_loss: 189.2139\n",
      "Epoch 16/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 199.3619 - val_loss: 175.8313\n",
      "Epoch 17/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 189.7843 - val_loss: 185.4275\n",
      "Epoch 18/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 198.4610 - val_loss: 189.8811\n",
      "Epoch 19/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 188.8173 - val_loss: 181.4314\n",
      "Epoch 20/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 188.9111 - val_loss: 192.6810\n",
      "Epoch 21/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 189.6833 - val_loss: 182.1473\n",
      "Epoch 22/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 191.3887 - val_loss: 219.6453\n",
      "Epoch 23/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 186.9991 - val_loss: 203.6355\n",
      "Epoch 24/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 183.9594 - val_loss: 194.1827\n",
      "Epoch 25/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 188.7456 - val_loss: 205.8199\n",
      "Epoch 26/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 185.1486 - val_loss: 215.7195\n",
      "Epoch 27/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 194.3553 - val_loss: 203.7359\n",
      "Epoch 28/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 187.8126 - val_loss: 202.6814\n",
      "Epoch 29/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 189.2321 - val_loss: 210.2978\n",
      "Epoch 30/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 177.0822 - val_loss: 207.9925\n",
      "Epoch 31/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 178.9202 - val_loss: 208.2249\n",
      "Epoch 32/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 176.9790 - val_loss: 202.4017\n",
      "Epoch 33/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 174.6391 - val_loss: 201.5517\n",
      "Epoch 34/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 174.8743 - val_loss: 209.1360\n",
      "Epoch 35/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 174.0004 - val_loss: 220.7057\n",
      "Epoch 36/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 183.3054 - val_loss: 208.3506\n",
      "Epoch 37/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 170.9109 - val_loss: 213.2901\n",
      "Epoch 38/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 174.1696 - val_loss: 220.4275\n",
      "Epoch 39/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 172.6884 - val_loss: 219.5252\n",
      "Epoch 40/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 170.3024 - val_loss: 226.8779\n",
      "Epoch 41/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 171.1201 - val_loss: 213.2518\n",
      "Epoch 42/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 169.3515 - val_loss: 216.9349\n",
      "Epoch 43/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 170.9217 - val_loss: 231.0482\n",
      "Epoch 44/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 169.3291 - val_loss: 223.6026\n",
      "Epoch 45/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 166.9460 - val_loss: 232.1920\n",
      "Epoch 46/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 172.3212 - val_loss: 252.0743\n",
      "Epoch 47/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 164.6772 - val_loss: 222.6779\n",
      "Epoch 48/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 165.2594 - val_loss: 223.3567\n",
      "Epoch 49/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 162.4103 - val_loss: 237.8644\n",
      "Epoch 50/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 164.1852 - val_loss: 221.4690\n",
      "Epoch 51/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 167.5369 - val_loss: 231.5667\n",
      "Epoch 52/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 165.8252 - val_loss: 231.2847\n",
      "Epoch 53/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 163.7955 - val_loss: 220.0991\n",
      "Epoch 54/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 164.2857 - val_loss: 228.0007\n",
      "Epoch 55/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 163.6814 - val_loss: 232.6951\n",
      "Epoch 56/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 159.8973 - val_loss: 261.2090\n",
      "Epoch 57/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 172.8352 - val_loss: 236.0093\n",
      "Epoch 58/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 161.1191 - val_loss: 225.9977\n",
      "Epoch 59/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 153.1222 - val_loss: 248.8212\n",
      "Epoch 60/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 175.1823 - val_loss: 227.7540\n",
      "Epoch 61/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 155.9641 - val_loss: 233.2244\n",
      "Epoch 62/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 154.4130 - val_loss: 233.0291\n",
      "Epoch 63/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 153.0775 - val_loss: 239.2415\n",
      "Epoch 64/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 152.4437 - val_loss: 277.4958\n",
      "Epoch 65/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 156.4483 - val_loss: 237.6090\n",
      "Epoch 66/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 152.5859 - val_loss: 245.7102\n",
      "Epoch 67/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 154.8785 - val_loss: 257.1086\n",
      "Epoch 68/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 155.9744 - val_loss: 234.7631\n",
      "Epoch 69/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 148.5945 - val_loss: 282.0984\n",
      "Epoch 70/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 153.1660 - val_loss: 232.3677\n",
      "Epoch 71/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 154.9655 - val_loss: 242.5027\n",
      "Epoch 72/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 146.0611 - val_loss: 242.9169\n",
      "Epoch 73/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 147.4584 - val_loss: 248.2968\n",
      "Epoch 74/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 147.9096 - val_loss: 245.1485\n",
      "Epoch 75/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 150.9324 - val_loss: 247.3565\n",
      "Epoch 76/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 149.5550 - val_loss: 243.2385\n",
      "Epoch 77/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 146.3648 - val_loss: 243.2289\n",
      "Epoch 78/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 145.5393 - val_loss: 249.7192\n",
      "Epoch 79/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 144.5670 - val_loss: 254.5753\n",
      "Epoch 80/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 143.4954 - val_loss: 267.5720\n",
      "Epoch 81/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 147.7737 - val_loss: 247.4512\n",
      "Epoch 82/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 142.5759 - val_loss: 243.2207\n",
      "Epoch 83/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 140.6812 - val_loss: 241.0023\n",
      "Epoch 84/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 143.4021 - val_loss: 252.3353\n",
      "Epoch 85/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 152.0735 - val_loss: 287.8116\n",
      "Epoch 86/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 159.0912 - val_loss: 275.9706\n",
      "Epoch 87/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 146.0058 - val_loss: 256.4881\n",
      "Epoch 88/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 137.8461 - val_loss: 268.1221\n",
      "Epoch 89/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 140.4304 - val_loss: 255.8873\n",
      "Epoch 90/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 145.7832 - val_loss: 250.5058\n",
      "Epoch 91/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 138.3092 - val_loss: 283.8353\n",
      "Epoch 92/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 137.5712 - val_loss: 260.7003\n",
      "Epoch 93/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 140.8446 - val_loss: 268.1196\n",
      "Epoch 94/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 137.4625 - val_loss: 264.1802\n",
      "Epoch 95/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 138.0301 - val_loss: 263.1880\n",
      "Epoch 96/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 133.6245 - val_loss: 252.1837\n",
      "Epoch 97/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 140.8348 - val_loss: 266.2233\n",
      "Epoch 98/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 133.5850 - val_loss: 254.1439\n",
      "Epoch 99/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 134.0342 - val_loss: 260.9658\n",
      "Epoch 100/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 135.7352 - val_loss: 259.8617\n",
      "Epoch 101/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 135.7369 - val_loss: 271.4407\n",
      "Epoch 102/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 146.0434 - val_loss: 262.7865\n",
      "Epoch 103/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 132.6546 - val_loss: 283.8140\n",
      "Epoch 104/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 136.6881 - val_loss: 273.2397\n",
      "Epoch 105/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 136.6506 - val_loss: 287.1826\n",
      "Epoch 106/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 132.9562 - val_loss: 271.2042\n",
      "Epoch 107/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 134.7144 - val_loss: 286.2397\n",
      "Epoch 108/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 132.6952 - val_loss: 276.8038\n",
      "Epoch 109/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 131.0712 - val_loss: 276.8633\n",
      "Epoch 110/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 139.2135 - val_loss: 269.1086\n",
      "Epoch 111/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 133.4326 - val_loss: 276.0350\n",
      "Epoch 112/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 129.1996 - val_loss: 285.2155\n",
      "Epoch 113/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 135.1079 - val_loss: 297.5717\n",
      "Epoch 114/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 133.6678 - val_loss: 269.1939\n",
      "Epoch 115/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 128.7849 - val_loss: 274.1612\n",
      "Epoch 116/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 134.3316 - val_loss: 266.2165\n",
      "Epoch 117/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 127.4289 - val_loss: 272.0798\n",
      "Epoch 118/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 128.0117 - val_loss: 279.9182\n",
      "Epoch 119/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 134.2508 - val_loss: 286.0747\n",
      "Epoch 120/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 130.0548 - val_loss: 283.5887\n",
      "Epoch 121/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 126.3742 - val_loss: 280.3592\n",
      "Epoch 122/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 126.9186 - val_loss: 315.9523\n",
      "Epoch 123/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 133.9602 - val_loss: 281.7074\n",
      "Epoch 124/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 124.9613 - val_loss: 277.8038\n",
      "Epoch 125/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 122.6120 - val_loss: 289.9155\n",
      "Epoch 126/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 120.9031 - val_loss: 311.4218\n",
      "Epoch 127/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 132.4917 - val_loss: 281.5988\n",
      "Epoch 128/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 126.5481 - val_loss: 277.0679\n",
      "Epoch 129/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 119.5892 - val_loss: 320.5439\n",
      "Epoch 130/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 130.4422 - val_loss: 301.4739\n",
      "Epoch 131/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 119.4537 - val_loss: 285.7468\n",
      "Epoch 132/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 121.8573 - val_loss: 289.6946\n",
      "Epoch 133/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 119.1249 - val_loss: 286.9015\n",
      "Epoch 134/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 124.3429 - val_loss: 283.4277\n",
      "Epoch 135/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 125.7254 - val_loss: 301.6449\n",
      "Epoch 136/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 118.6383 - val_loss: 288.9953\n",
      "Epoch 137/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 122.4201 - val_loss: 289.2079\n",
      "Epoch 138/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 121.4019 - val_loss: 296.3100\n",
      "Epoch 139/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 124.2684 - val_loss: 296.6091\n",
      "Epoch 140/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 119.4089 - val_loss: 297.2511\n",
      "Epoch 141/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 115.5868 - val_loss: 290.9641\n",
      "Epoch 142/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 119.7364 - val_loss: 294.0575\n",
      "Epoch 143/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 118.2030 - val_loss: 290.9356\n",
      "Epoch 144/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 117.6117 - val_loss: 285.4875\n",
      "Epoch 145/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 116.2849 - val_loss: 300.5547\n",
      "Epoch 146/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 114.9164 - val_loss: 292.2152\n",
      "Epoch 147/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 117.7583 - val_loss: 288.0937\n",
      "Epoch 148/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 113.6587 - val_loss: 292.0136\n",
      "Epoch 149/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 117.0945 - val_loss: 335.6462\n",
      "Epoch 150/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 119.4822 - val_loss: 304.3129\n",
      "Epoch 151/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 116.2935 - val_loss: 310.6652\n",
      "Epoch 152/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 111.9033 - val_loss: 330.7258\n",
      "Epoch 153/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 120.2556 - val_loss: 295.3098\n",
      "Epoch 154/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 112.7429 - val_loss: 322.0297\n",
      "Epoch 155/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 112.8217 - val_loss: 309.3105\n",
      "Epoch 156/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 117.2331 - val_loss: 298.5253\n",
      "Epoch 157/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 112.1733 - val_loss: 291.4483\n",
      "Epoch 158/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 112.8985 - val_loss: 318.1897\n",
      "Epoch 159/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 120.6872 - val_loss: 293.8630\n",
      "Epoch 160/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 110.1762 - val_loss: 297.4252\n",
      "Epoch 161/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 112.1275 - val_loss: 305.4326\n",
      "Epoch 162/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 119.4644 - val_loss: 300.9939\n",
      "Epoch 163/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 109.1974 - val_loss: 330.6300\n",
      "Epoch 164/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 112.6024 - val_loss: 305.7464\n",
      "Epoch 165/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 111.9672 - val_loss: 295.6745\n",
      "Epoch 166/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 105.4591 - val_loss: 304.5793\n",
      "Epoch 167/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 112.3460 - val_loss: 327.4019\n",
      "Epoch 168/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 108.1020 - val_loss: 301.8457\n",
      "Epoch 169/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 114.0513 - val_loss: 311.7913\n",
      "Epoch 170/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 108.1734 - val_loss: 342.4073\n",
      "Epoch 171/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 111.8306 - val_loss: 301.2945\n",
      "Epoch 172/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 105.8681 - val_loss: 317.7771\n",
      "Epoch 173/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 114.2280 - val_loss: 300.0266\n",
      "Epoch 174/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 109.4224 - val_loss: 309.6086\n",
      "Epoch 175/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 110.7592 - val_loss: 315.0905\n",
      "Epoch 176/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 113.4904 - val_loss: 310.3828\n",
      "Epoch 177/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 106.0873 - val_loss: 316.1341\n",
      "Epoch 178/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 105.1961 - val_loss: 335.9102\n",
      "Epoch 179/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 106.0377 - val_loss: 354.4417\n",
      "Epoch 180/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 116.7683 - val_loss: 312.7696\n",
      "Epoch 181/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 105.9928 - val_loss: 318.7269\n",
      "Epoch 182/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 105.6984 - val_loss: 312.1649\n",
      "Epoch 183/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 104.0874 - val_loss: 317.6381\n",
      "Epoch 184/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 105.9208 - val_loss: 330.4441\n",
      "Epoch 185/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 105.3118 - val_loss: 309.0776\n",
      "Epoch 186/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 105.4020 - val_loss: 318.2172\n",
      "Epoch 187/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 105.7687 - val_loss: 313.7393\n",
      "Epoch 188/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 103.2891 - val_loss: 356.4665\n",
      "Epoch 189/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 113.7631 - val_loss: 319.3986\n",
      "Epoch 190/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 101.3816 - val_loss: 314.9165\n",
      "Epoch 191/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 102.4086 - val_loss: 311.4612\n",
      "Epoch 192/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 102.0697 - val_loss: 323.8257\n",
      "Epoch 193/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 104.7906 - val_loss: 315.9170\n",
      "Epoch 194/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 109.1558 - val_loss: 316.9474\n",
      "Epoch 195/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 108.0393 - val_loss: 337.4291\n",
      "Epoch 196/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 103.5686 - val_loss: 325.7709\n",
      "Epoch 197/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 101.4520 - val_loss: 333.6848\n",
      "Epoch 198/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 102.8509 - val_loss: 319.0977\n",
      "Epoch 199/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 100.6898 - val_loss: 327.1976\n",
      "Epoch 200/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 99.7334 - val_loss: 332.2968\n",
      "Epoch 1/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 174.0786 - val_loss: 137.6404\n",
      "Epoch 2/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 186.3486 - val_loss: 131.5779\n",
      "Epoch 3/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 161.4610 - val_loss: 118.1488\n",
      "Epoch 4/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 155.7953 - val_loss: 110.0699\n",
      "Epoch 5/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 143.2598 - val_loss: 109.9054\n",
      "Epoch 6/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 140.8516 - val_loss: 113.9172\n",
      "Epoch 7/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 142.2490 - val_loss: 133.5443\n",
      "Epoch 8/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 144.2189 - val_loss: 138.5759\n",
      "Epoch 9/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 138.7780 - val_loss: 119.4695\n",
      "Epoch 10/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 135.2231 - val_loss: 138.9428\n",
      "Epoch 11/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 139.6235 - val_loss: 130.8858\n",
      "Epoch 12/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 131.2543 - val_loss: 149.5607\n",
      "Epoch 13/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 137.2966 - val_loss: 128.1167\n",
      "Epoch 14/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 134.4992 - val_loss: 129.9469\n",
      "Epoch 15/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 129.8870 - val_loss: 142.3726\n",
      "Epoch 16/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 132.9131 - val_loss: 129.4518\n",
      "Epoch 17/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 137.3849 - val_loss: 144.4481\n",
      "Epoch 18/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 130.6538 - val_loss: 148.8073\n",
      "Epoch 19/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 136.9095 - val_loss: 160.7652\n",
      "Epoch 20/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 125.3192 - val_loss: 136.6152\n",
      "Epoch 21/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 132.2024 - val_loss: 155.9291\n",
      "Epoch 22/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 126.9821 - val_loss: 157.0539\n",
      "Epoch 23/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 128.5052 - val_loss: 146.9113\n",
      "Epoch 24/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 131.2148 - val_loss: 147.5673\n",
      "Epoch 25/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 118.9752 - val_loss: 136.8359\n",
      "Epoch 26/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 122.7732 - val_loss: 150.6126\n",
      "Epoch 27/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 121.9021 - val_loss: 143.0569\n",
      "Epoch 28/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 123.2004 - val_loss: 185.3852\n",
      "Epoch 29/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 129.1611 - val_loss: 167.0870\n",
      "Epoch 30/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 120.5414 - val_loss: 154.3520\n",
      "Epoch 31/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 123.5337 - val_loss: 175.1496\n",
      "Epoch 32/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 118.0564 - val_loss: 159.4345\n",
      "Epoch 33/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 122.9856 - val_loss: 151.0873\n",
      "Epoch 34/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 120.0557 - val_loss: 163.3444\n",
      "Epoch 35/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 116.1381 - val_loss: 161.7882\n",
      "Epoch 36/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 117.6573 - val_loss: 156.8241\n",
      "Epoch 37/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 114.9423 - val_loss: 275.8464\n",
      "Epoch 38/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 120.3665 - val_loss: 156.5287\n",
      "Epoch 39/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 114.2922 - val_loss: 161.4142\n",
      "Epoch 40/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 112.8850 - val_loss: 156.3508\n",
      "Epoch 41/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 115.8681 - val_loss: 172.2804\n",
      "Epoch 42/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 120.4472 - val_loss: 162.4471\n",
      "Epoch 43/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 115.9972 - val_loss: 187.7402\n",
      "Epoch 44/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 116.3732 - val_loss: 161.1859\n",
      "Epoch 45/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 112.4294 - val_loss: 159.8443\n",
      "Epoch 46/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 110.8406 - val_loss: 182.9554\n",
      "Epoch 47/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 116.7248 - val_loss: 169.8094\n",
      "Epoch 48/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 109.3274 - val_loss: 165.9649\n",
      "Epoch 49/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 109.4301 - val_loss: 171.5817\n",
      "Epoch 50/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 116.4196 - val_loss: 173.8420\n",
      "Epoch 51/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 110.4120 - val_loss: 164.0537\n",
      "Epoch 52/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 112.1281 - val_loss: 227.4613\n",
      "Epoch 53/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 118.3211 - val_loss: 184.9530\n",
      "Epoch 54/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 114.4920 - val_loss: 238.6306\n",
      "Epoch 55/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 115.7395 - val_loss: 169.7400\n",
      "Epoch 56/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 106.8648 - val_loss: 166.0316\n",
      "Epoch 57/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 106.7675 - val_loss: 175.1474\n",
      "Epoch 58/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 111.1523 - val_loss: 192.9588\n",
      "Epoch 59/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 108.2270 - val_loss: 183.5415\n",
      "Epoch 60/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 108.8629 - val_loss: 174.1346\n",
      "Epoch 61/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 109.5111 - val_loss: 170.2726\n",
      "Epoch 62/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 106.6957 - val_loss: 195.5579\n",
      "Epoch 63/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 104.5957 - val_loss: 187.2313\n",
      "Epoch 64/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 104.7169 - val_loss: 189.3474\n",
      "Epoch 65/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 104.2182 - val_loss: 176.5393\n",
      "Epoch 66/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 101.9651 - val_loss: 234.7605\n",
      "Epoch 67/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 117.5773 - val_loss: 183.2249\n",
      "Epoch 68/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 104.7692 - val_loss: 189.4162\n",
      "Epoch 69/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 101.6972 - val_loss: 191.1210\n",
      "Epoch 70/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 100.1527 - val_loss: 186.9782\n",
      "Epoch 71/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 102.7523 - val_loss: 238.3299\n",
      "Epoch 72/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 120.3938 - val_loss: 182.6632\n",
      "Epoch 73/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 101.6391 - val_loss: 185.9290\n",
      "Epoch 74/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 104.0374 - val_loss: 206.4587\n",
      "Epoch 75/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 101.5203 - val_loss: 186.2683\n",
      "Epoch 76/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 102.0132 - val_loss: 213.5601\n",
      "Epoch 77/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 102.9019 - val_loss: 179.3407\n",
      "Epoch 78/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 98.9613 - val_loss: 181.3745\n",
      "Epoch 79/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 99.9663 - val_loss: 196.6725\n",
      "Epoch 80/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 101.6836 - val_loss: 184.4945\n",
      "Epoch 81/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 99.0736 - val_loss: 187.7513\n",
      "Epoch 82/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 101.2022 - val_loss: 199.9290\n",
      "Epoch 83/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 104.3808 - val_loss: 184.5473\n",
      "Epoch 84/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 96.8234 - val_loss: 189.7831\n",
      "Epoch 85/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 100.4605 - val_loss: 204.0552\n",
      "Epoch 86/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 103.3816 - val_loss: 188.2706\n",
      "Epoch 87/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 102.8988 - val_loss: 208.6252\n",
      "Epoch 88/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 93.8274 - val_loss: 188.1782\n",
      "Epoch 89/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 95.1395 - val_loss: 199.2814\n",
      "Epoch 90/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 96.1212 - val_loss: 194.8244\n",
      "Epoch 91/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 98.5126 - val_loss: 189.2277\n",
      "Epoch 92/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 100.9497 - val_loss: 201.1971\n",
      "Epoch 93/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 97.0886 - val_loss: 192.6971\n",
      "Epoch 94/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 95.3698 - val_loss: 197.0427\n",
      "Epoch 95/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 94.4856 - val_loss: 212.2929\n",
      "Epoch 96/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 99.8060 - val_loss: 194.9387\n",
      "Epoch 97/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 99.8781 - val_loss: 208.8114\n",
      "Epoch 98/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 104.1445 - val_loss: 212.9534\n",
      "Epoch 99/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 92.1968 - val_loss: 194.0589\n",
      "Epoch 100/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 91.8168 - val_loss: 194.2091\n",
      "Epoch 101/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 94.5149 - val_loss: 230.0101\n",
      "Epoch 102/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 100.5330 - val_loss: 206.4861\n",
      "Epoch 103/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 92.4415 - val_loss: 206.7975\n",
      "Epoch 104/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 99.1018 - val_loss: 199.1290\n",
      "Epoch 105/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 91.7268 - val_loss: 202.3048\n",
      "Epoch 106/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 90.6543 - val_loss: 208.5424\n",
      "Epoch 107/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 90.0994 - val_loss: 207.9714\n",
      "Epoch 108/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 93.5898 - val_loss: 207.2787\n",
      "Epoch 109/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 98.4994 - val_loss: 228.3602\n",
      "Epoch 110/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 99.1337 - val_loss: 203.2743\n",
      "Epoch 111/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 91.3686 - val_loss: 221.8029\n",
      "Epoch 112/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 94.9969 - val_loss: 204.0168\n",
      "Epoch 113/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 87.1884 - val_loss: 209.7226\n",
      "Epoch 114/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 87.8692 - val_loss: 200.8364\n",
      "Epoch 115/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 87.3104 - val_loss: 208.9502\n",
      "Epoch 116/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 96.6304 - val_loss: 240.4855\n",
      "Epoch 117/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 96.4467 - val_loss: 212.5578\n",
      "Epoch 118/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 86.4090 - val_loss: 218.8210\n",
      "Epoch 119/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 92.3654 - val_loss: 211.8390\n",
      "Epoch 120/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 88.2798 - val_loss: 227.1530\n",
      "Epoch 121/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 91.6959 - val_loss: 219.1678\n",
      "Epoch 122/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 87.5063 - val_loss: 220.6256\n",
      "Epoch 123/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 86.8911 - val_loss: 216.0361\n",
      "Epoch 124/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 90.6182 - val_loss: 207.9425\n",
      "Epoch 125/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 93.4590 - val_loss: 228.1548\n",
      "Epoch 126/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 103.0876 - val_loss: 214.5544\n",
      "Epoch 127/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 84.8388 - val_loss: 263.9814\n",
      "Epoch 128/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 93.8309 - val_loss: 212.7437\n",
      "Epoch 129/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 85.5458 - val_loss: 261.1764\n",
      "Epoch 130/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 86.0939 - val_loss: 223.4115\n",
      "Epoch 131/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 87.5700 - val_loss: 212.0436\n",
      "Epoch 132/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 87.1001 - val_loss: 206.2380\n",
      "Epoch 133/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 84.1799 - val_loss: 207.0176\n",
      "Epoch 134/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 85.8520 - val_loss: 218.7219\n",
      "Epoch 135/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 87.6352 - val_loss: 264.5959\n",
      "Epoch 136/200\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 92.3815 - val_loss: 223.7612\n",
      "Epoch 137/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 84.4545 - val_loss: 237.3881\n",
      "Epoch 138/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 85.1063 - val_loss: 226.0906\n",
      "Epoch 139/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 86.5356 - val_loss: 213.6502\n",
      "Epoch 140/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 88.7299 - val_loss: 217.7506\n",
      "Epoch 141/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 84.6246 - val_loss: 255.8870\n",
      "Epoch 142/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 99.2152 - val_loss: 223.9716\n",
      "Epoch 143/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 86.1302 - val_loss: 231.1680\n",
      "Epoch 144/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 92.4700 - val_loss: 208.9200\n",
      "Epoch 145/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 84.5768 - val_loss: 228.1172\n",
      "Epoch 146/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 84.6633 - val_loss: 222.6976\n",
      "Epoch 147/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 81.1977 - val_loss: 228.3396\n",
      "Epoch 148/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 86.2949 - val_loss: 217.9949\n",
      "Epoch 149/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 86.3356 - val_loss: 228.7063\n",
      "Epoch 150/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 87.8116 - val_loss: 248.3672\n",
      "Epoch 151/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 81.7193 - val_loss: 218.5102\n",
      "Epoch 152/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 81.6636 - val_loss: 238.0433\n",
      "Epoch 153/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 86.2579 - val_loss: 229.1289\n",
      "Epoch 154/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 84.1222 - val_loss: 216.2451\n",
      "Epoch 155/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 79.4248 - val_loss: 222.6796\n",
      "Epoch 156/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 85.7771 - val_loss: 225.3812\n",
      "Epoch 157/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 87.6126 - val_loss: 226.7266\n",
      "Epoch 158/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 81.6923 - val_loss: 216.9698\n",
      "Epoch 159/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 79.6175 - val_loss: 233.4781\n",
      "Epoch 160/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 84.0774 - val_loss: 220.4513\n",
      "Epoch 161/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 81.6001 - val_loss: 232.3845\n",
      "Epoch 162/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 85.5893 - val_loss: 246.8767\n",
      "Epoch 163/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 78.5191 - val_loss: 231.1317\n",
      "Epoch 164/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 85.2929 - val_loss: 230.6288\n",
      "Epoch 165/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 82.1027 - val_loss: 247.0796\n",
      "Epoch 166/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 89.2271 - val_loss: 222.2046\n",
      "Epoch 167/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 79.8736 - val_loss: 221.6087\n",
      "Epoch 168/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 81.3988 - val_loss: 221.5561\n",
      "Epoch 169/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 76.3004 - val_loss: 238.3063\n",
      "Epoch 170/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 81.2673 - val_loss: 246.3422\n",
      "Epoch 171/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 83.4835 - val_loss: 234.5159\n",
      "Epoch 172/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 79.1246 - val_loss: 234.4887\n",
      "Epoch 173/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 79.0431 - val_loss: 236.8404\n",
      "Epoch 174/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 81.9643 - val_loss: 235.0240\n",
      "Epoch 175/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 79.2640 - val_loss: 229.5361\n",
      "Epoch 176/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 80.7533 - val_loss: 222.8988\n",
      "Epoch 177/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 77.9924 - val_loss: 239.8876\n",
      "Epoch 178/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 78.7074 - val_loss: 244.3099\n",
      "Epoch 179/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 78.6654 - val_loss: 282.5759\n",
      "Epoch 180/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 83.8184 - val_loss: 228.5995\n",
      "Epoch 181/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 77.7475 - val_loss: 225.7539\n",
      "Epoch 182/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 77.6820 - val_loss: 247.8858\n",
      "Epoch 183/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 79.7238 - val_loss: 235.8049\n",
      "Epoch 184/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 82.8916 - val_loss: 246.6245\n",
      "Epoch 185/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 88.3065 - val_loss: 233.0925\n",
      "Epoch 186/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 78.8117 - val_loss: 235.2615\n",
      "Epoch 187/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 75.2292 - val_loss: 229.3914\n",
      "Epoch 188/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 75.7259 - val_loss: 227.4390\n",
      "Epoch 189/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 74.1755 - val_loss: 233.1753\n",
      "Epoch 190/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 77.6855 - val_loss: 248.7309\n",
      "Epoch 191/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 82.0728 - val_loss: 256.3447\n",
      "Epoch 192/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 78.8156 - val_loss: 231.8857\n",
      "Epoch 193/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 77.3499 - val_loss: 234.0219\n",
      "Epoch 194/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 73.0459 - val_loss: 261.1442\n",
      "Epoch 195/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 91.1730 - val_loss: 244.2944\n",
      "Epoch 196/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 72.4118 - val_loss: 234.8459\n",
      "Epoch 197/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 76.4682 - val_loss: 238.3295\n",
      "Epoch 198/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 84.1022 - val_loss: 268.5294\n",
      "Epoch 199/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 76.3983 - val_loss: 236.2116\n",
      "Epoch 200/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 74.5148 - val_loss: 246.8301\n",
      "Epoch 1/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 137.9791 - val_loss: 96.9099\n",
      "Epoch 2/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 134.4164 - val_loss: 114.7777\n",
      "Epoch 3/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 124.1034 - val_loss: 95.9796\n",
      "Epoch 4/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 116.5154 - val_loss: 98.6282\n",
      "Epoch 5/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 119.9119 - val_loss: 96.4438\n",
      "Epoch 6/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 110.2602 - val_loss: 95.5286\n",
      "Epoch 7/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 108.7862 - val_loss: 127.0173\n",
      "Epoch 8/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 104.3590 - val_loss: 88.4966\n",
      "Epoch 9/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 109.7502 - val_loss: 107.7423\n",
      "Epoch 10/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 109.2346 - val_loss: 94.8168\n",
      "Epoch 11/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 101.7982 - val_loss: 96.3570\n",
      "Epoch 12/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 104.3126 - val_loss: 88.9829\n",
      "Epoch 13/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 99.3750 - val_loss: 112.9083\n",
      "Epoch 14/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 116.5996 - val_loss: 90.5087\n",
      "Epoch 15/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 103.9182 - val_loss: 105.5348\n",
      "Epoch 16/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 96.9967 - val_loss: 91.5990\n",
      "Epoch 17/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 100.0390 - val_loss: 119.2421\n",
      "Epoch 18/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 115.9575 - val_loss: 115.7508\n",
      "Epoch 19/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 94.6709 - val_loss: 92.3513\n",
      "Epoch 20/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 92.3407 - val_loss: 104.3603\n",
      "Epoch 21/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 99.3591 - val_loss: 125.9857\n",
      "Epoch 22/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 95.4608 - val_loss: 112.9177\n",
      "Epoch 23/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 94.0670 - val_loss: 94.9158\n",
      "Epoch 24/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 89.8007 - val_loss: 105.2808\n",
      "Epoch 25/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 99.6559 - val_loss: 122.8738\n",
      "Epoch 26/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 98.2032 - val_loss: 111.8690\n",
      "Epoch 27/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 90.9938 - val_loss: 171.2799\n",
      "Epoch 28/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 91.9211 - val_loss: 131.7730\n",
      "Epoch 29/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 90.6217 - val_loss: 143.3922\n",
      "Epoch 30/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 96.5697 - val_loss: 119.4095\n",
      "Epoch 31/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 90.0540 - val_loss: 131.1302\n",
      "Epoch 32/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 92.0298 - val_loss: 128.9150\n",
      "Epoch 33/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 91.0058 - val_loss: 112.5596\n",
      "Epoch 34/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 92.3223 - val_loss: 116.2394\n",
      "Epoch 35/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 95.3841 - val_loss: 112.5563\n",
      "Epoch 36/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 90.9991 - val_loss: 115.8957\n",
      "Epoch 37/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 93.7428 - val_loss: 116.3100\n",
      "Epoch 38/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 85.8594 - val_loss: 135.0755\n",
      "Epoch 39/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 93.7253 - val_loss: 112.9916\n",
      "Epoch 40/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 90.9421 - val_loss: 117.8523\n",
      "Epoch 41/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 88.3160 - val_loss: 115.6373\n",
      "Epoch 42/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 84.3251 - val_loss: 135.1611\n",
      "Epoch 43/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 93.0584 - val_loss: 118.0527\n",
      "Epoch 44/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 81.2983 - val_loss: 119.3376\n",
      "Epoch 45/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 80.8681 - val_loss: 116.5058\n",
      "Epoch 46/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 85.6498 - val_loss: 130.3421\n",
      "Epoch 47/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 85.4269 - val_loss: 149.8236\n",
      "Epoch 48/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 90.7916 - val_loss: 129.2503\n",
      "Epoch 49/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 85.6926 - val_loss: 123.2855\n",
      "Epoch 50/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 83.0309 - val_loss: 125.7112\n",
      "Epoch 51/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 83.5200 - val_loss: 192.7733\n",
      "Epoch 52/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 83.2897 - val_loss: 122.5270\n",
      "Epoch 53/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 85.3753 - val_loss: 122.4361\n",
      "Epoch 54/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 84.5118 - val_loss: 123.8936\n",
      "Epoch 55/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 81.6293 - val_loss: 131.3451\n",
      "Epoch 56/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 84.2598 - val_loss: 129.5921\n",
      "Epoch 57/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 88.1211 - val_loss: 154.9938\n",
      "Epoch 58/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 90.6052 - val_loss: 121.4966\n",
      "Epoch 59/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 83.0996 - val_loss: 125.3295\n",
      "Epoch 60/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 80.9951 - val_loss: 154.3882\n",
      "Epoch 61/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 89.0105 - val_loss: 122.5409\n",
      "Epoch 62/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 78.8993 - val_loss: 130.3298\n",
      "Epoch 63/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 77.3789 - val_loss: 128.0989\n",
      "Epoch 64/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 77.9119 - val_loss: 128.1256\n",
      "Epoch 65/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 76.9282 - val_loss: 131.2341\n",
      "Epoch 66/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 85.6497 - val_loss: 139.5920\n",
      "Epoch 67/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 81.8044 - val_loss: 130.1280\n",
      "Epoch 68/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 81.2018 - val_loss: 148.3609\n",
      "Epoch 69/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 79.1104 - val_loss: 134.6006\n",
      "Epoch 70/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 78.6965 - val_loss: 132.4668\n",
      "Epoch 71/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 80.5961 - val_loss: 138.7118\n",
      "Epoch 72/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 76.0086 - val_loss: 133.6525\n",
      "Epoch 73/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 79.9820 - val_loss: 161.7819\n",
      "Epoch 74/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 79.2936 - val_loss: 135.9739\n",
      "Epoch 75/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 78.4364 - val_loss: 153.1467\n",
      "Epoch 76/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 78.0956 - val_loss: 150.2093\n",
      "Epoch 77/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 79.2342 - val_loss: 138.2440\n",
      "Epoch 78/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 79.7183 - val_loss: 143.1571\n",
      "Epoch 79/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 79.6188 - val_loss: 134.1039\n",
      "Epoch 80/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 75.3435 - val_loss: 133.4283\n",
      "Epoch 81/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 72.0316 - val_loss: 148.5565\n",
      "Epoch 82/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 78.9841 - val_loss: 133.5733\n",
      "Epoch 83/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 79.8035 - val_loss: 146.1740\n",
      "Epoch 84/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 75.6333 - val_loss: 140.5020\n",
      "Epoch 85/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 81.1413 - val_loss: 165.4620\n",
      "Epoch 86/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 91.7489 - val_loss: 148.2055\n",
      "Epoch 87/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 76.8656 - val_loss: 146.3443\n",
      "Epoch 88/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 73.4042 - val_loss: 150.0974\n",
      "Epoch 89/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 76.3436 - val_loss: 136.4723\n",
      "Epoch 90/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 71.8664 - val_loss: 151.4890\n",
      "Epoch 91/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 76.9400 - val_loss: 154.7184\n",
      "Epoch 92/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 77.6827 - val_loss: 159.4930\n",
      "Epoch 93/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 73.3414 - val_loss: 171.1261\n",
      "Epoch 94/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 73.7174 - val_loss: 149.3606\n",
      "Epoch 95/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 72.4688 - val_loss: 145.5801\n",
      "Epoch 96/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 78.3172 - val_loss: 166.6459\n",
      "Epoch 97/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 82.6369 - val_loss: 163.1558\n",
      "Epoch 98/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 81.8255 - val_loss: 154.9741\n",
      "Epoch 99/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 78.5277 - val_loss: 153.9076\n",
      "Epoch 100/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 73.1282 - val_loss: 143.4306\n",
      "Epoch 101/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 73.4251 - val_loss: 243.4566\n",
      "Epoch 102/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 80.7902 - val_loss: 173.4166\n",
      "Epoch 103/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 80.9108 - val_loss: 143.3179\n",
      "Epoch 104/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 73.3876 - val_loss: 153.8929\n",
      "Epoch 105/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 69.9187 - val_loss: 151.0446\n",
      "Epoch 106/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 76.3231 - val_loss: 183.8380\n",
      "Epoch 107/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 72.5518 - val_loss: 153.7440\n",
      "Epoch 108/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 75.5078 - val_loss: 144.0645\n",
      "Epoch 109/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 68.2520 - val_loss: 170.9564\n",
      "Epoch 110/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 84.7743 - val_loss: 156.5374\n",
      "Epoch 111/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 71.1581 - val_loss: 143.8040\n",
      "Epoch 112/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 70.8372 - val_loss: 158.8366\n",
      "Epoch 113/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 71.4794 - val_loss: 153.6235\n",
      "Epoch 114/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 69.3133 - val_loss: 182.4526\n",
      "Epoch 115/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 84.2934 - val_loss: 149.4104\n",
      "Epoch 116/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 68.8516 - val_loss: 149.2784\n",
      "Epoch 117/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 65.4368 - val_loss: 172.8930\n",
      "Epoch 118/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 76.2952 - val_loss: 148.4908\n",
      "Epoch 119/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 72.3321 - val_loss: 170.0961\n",
      "Epoch 120/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 76.4423 - val_loss: 169.9084\n",
      "Epoch 121/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 71.8410 - val_loss: 154.0626\n",
      "Epoch 122/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 69.0678 - val_loss: 162.3802\n",
      "Epoch 123/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 72.8420 - val_loss: 166.3146\n",
      "Epoch 124/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 66.9477 - val_loss: 173.1600\n",
      "Epoch 125/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 68.5760 - val_loss: 164.8987\n",
      "Epoch 126/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 76.0346 - val_loss: 177.1026\n",
      "Epoch 127/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 75.1152 - val_loss: 155.4013\n",
      "Epoch 128/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 70.4591 - val_loss: 162.1457\n",
      "Epoch 129/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 69.2684 - val_loss: 164.2372\n",
      "Epoch 130/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 69.9967 - val_loss: 156.9636\n",
      "Epoch 131/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 74.1047 - val_loss: 155.5258\n",
      "Epoch 132/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 65.8211 - val_loss: 172.0430\n",
      "Epoch 133/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 70.5242 - val_loss: 164.9388\n",
      "Epoch 134/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 68.4864 - val_loss: 163.9079\n",
      "Epoch 135/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 66.1383 - val_loss: 175.6899\n",
      "Epoch 136/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 66.7208 - val_loss: 167.0242\n",
      "Epoch 137/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 68.6219 - val_loss: 155.2648\n",
      "Epoch 138/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 67.9547 - val_loss: 173.2009\n",
      "Epoch 139/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 72.8054 - val_loss: 158.9960\n",
      "Epoch 140/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 71.3446 - val_loss: 168.2215\n",
      "Epoch 141/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 72.9960 - val_loss: 169.1381\n",
      "Epoch 142/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 69.7745 - val_loss: 159.5454\n",
      "Epoch 143/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 66.3954 - val_loss: 164.0714\n",
      "Epoch 144/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 70.1278 - val_loss: 172.1150\n",
      "Epoch 145/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 67.5897 - val_loss: 191.2708\n",
      "Epoch 146/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 72.5697 - val_loss: 170.3309\n",
      "Epoch 147/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 65.8282 - val_loss: 158.0523\n",
      "Epoch 148/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 68.9421 - val_loss: 161.9426\n",
      "Epoch 149/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 66.0917 - val_loss: 171.8999\n",
      "Epoch 150/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 65.5447 - val_loss: 170.7112\n",
      "Epoch 151/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 71.9028 - val_loss: 163.0847\n",
      "Epoch 152/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 67.1800 - val_loss: 176.8419\n",
      "Epoch 153/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 69.0669 - val_loss: 164.3181\n",
      "Epoch 154/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 62.2197 - val_loss: 179.4384\n",
      "Epoch 155/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 67.0613 - val_loss: 183.4279\n",
      "Epoch 156/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 76.4730 - val_loss: 165.1060\n",
      "Epoch 157/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 63.7142 - val_loss: 162.8718\n",
      "Epoch 158/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 65.4018 - val_loss: 169.2379\n",
      "Epoch 159/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 63.5551 - val_loss: 174.0101\n",
      "Epoch 160/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 67.5986 - val_loss: 163.9231\n",
      "Epoch 161/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 67.4021 - val_loss: 168.6852\n",
      "Epoch 162/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 64.8629 - val_loss: 245.1013\n",
      "Epoch 163/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 64.8274 - val_loss: 202.1750\n",
      "Epoch 164/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 71.8933 - val_loss: 169.4730\n",
      "Epoch 165/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 66.0284 - val_loss: 212.7819\n",
      "Epoch 166/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 77.5195 - val_loss: 174.8438\n",
      "Epoch 167/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 66.5722 - val_loss: 176.8963\n",
      "Epoch 168/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 62.3406 - val_loss: 169.5901\n",
      "Epoch 169/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 61.7202 - val_loss: 173.8429\n",
      "Epoch 170/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 65.0137 - val_loss: 172.0937\n",
      "Epoch 171/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 62.8477 - val_loss: 183.2434\n",
      "Epoch 172/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 63.5532 - val_loss: 181.8259\n",
      "Epoch 173/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 65.8977 - val_loss: 196.2352\n",
      "Epoch 174/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 63.9366 - val_loss: 198.4202\n",
      "Epoch 175/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 64.8387 - val_loss: 189.1985\n",
      "Epoch 176/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 63.4373 - val_loss: 175.9279\n",
      "Epoch 177/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 65.6110 - val_loss: 179.6854\n",
      "Epoch 178/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 62.0641 - val_loss: 175.6559\n",
      "Epoch 179/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 61.1291 - val_loss: 180.0116\n",
      "Epoch 180/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 64.5857 - val_loss: 171.2682\n",
      "Epoch 181/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 62.8847 - val_loss: 174.4086\n",
      "Epoch 182/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 62.1047 - val_loss: 181.6250\n",
      "Epoch 183/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 66.4844 - val_loss: 181.1260\n",
      "Epoch 184/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 64.8661 - val_loss: 179.3969\n",
      "Epoch 185/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 60.0935 - val_loss: 176.9596\n",
      "Epoch 186/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 65.7921 - val_loss: 181.7152\n",
      "Epoch 187/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 62.4677 - val_loss: 176.7040\n",
      "Epoch 188/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 62.8111 - val_loss: 192.0039\n",
      "Epoch 189/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 68.1681 - val_loss: 191.1665\n",
      "Epoch 190/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 63.9736 - val_loss: 188.6895\n",
      "Epoch 191/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 63.0765 - val_loss: 174.6643\n",
      "Epoch 192/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 60.5666 - val_loss: 185.2392\n",
      "Epoch 193/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 62.0866 - val_loss: 177.0775\n",
      "Epoch 194/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 63.1251 - val_loss: 208.4644\n",
      "Epoch 195/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 61.2392 - val_loss: 191.8736\n",
      "Epoch 196/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 61.7750 - val_loss: 176.7480\n",
      "Epoch 197/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 62.0156 - val_loss: 186.3893\n",
      "Epoch 198/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 69.9331 - val_loss: 201.2894\n",
      "Epoch 199/200\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 81.6095 - val_loss: 184.9073\n",
      "Epoch 200/200\n",
      "197/197 [==============================] - 1s 3ms/step - loss: 58.3064 - val_loss: 181.6490\n"
     ]
    }
   ],
   "source": [
    "# Define the number of splits for cross-validation\n",
    "n_splits = 5\n",
    "\n",
    "# Create a KFold object\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Iterate through the folds\n",
    "for train_index, val_index in kf.split(X_train_scaled):\n",
    "    train_index = train_index[train_index < len(X_train_scaled)]\n",
    "    val_index = val_index[val_index < len(X_train_scaled)]\n",
    "    # Split the data into training and validation sets\n",
    "    X_train_val, X_val = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "    y_train_val, y_val = y_train[train_index], y_train[val_index]\n",
    "\n",
    "    # Train the model on the current fold\n",
    "    model_regression_tuned.fit(X_train_val, y_train_val, epochs=200, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #early_stop= EarlyStopping(patience=20)\n",
    "\n",
    "# model_regression_tuned.fit(\n",
    "#     X_train_scaled, y_train,\n",
    "#     epochs=1000,\n",
    "#     #callbacks=[early_stop]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n",
      "35.77627944946289 -> 22\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "64.93634033203125 -> 40\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "48.98577880859375 -> 70\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "51.45063400268555 -> 50\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "34.00908660888672 -> 42\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "19.105844497680664 -> 37\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "74.19535064697266 -> 57\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "45.76628112792969 -> 70\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "63.071571350097656 -> 60\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "22.544095993041992 -> 52\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "42.19994354248047 -> 38\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "45.811275482177734 -> 46\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "40.17328643798828 -> 60\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "48.246177673339844 -> 58\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "57.221656799316406 -> 83\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "67.27281951904297 -> 3\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "24.51039695739746 -> 18\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "67.53177642822266 -> 81\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "43.20955276489258 -> 47\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "2.660287380218506 -> 43\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "42.87987518310547 -> 45\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "43.19464111328125 -> 45\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "69.75994873046875 -> 72\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "49.708282470703125 -> 54\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "43.030967712402344 -> 35\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "84.59478759765625 -> 63\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "47.327640533447266 -> 50\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "46.179500579833984 -> 73\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "64.08423614501953 -> 32\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "47.998836517333984 -> 55\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "0.0 -> 34\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "41.76878356933594 -> 61\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "32.66468048095703 -> 48\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "46.64741897583008 -> 56\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "46.52768325805664 -> 73\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "54.90652847290039 -> 16\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "0.0 -> 42\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "25.48564338684082 -> 54\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "53.718570709228516 -> 73\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "32.66441345214844 -> 48\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "50.912933349609375 -> 55\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "54.657447814941406 -> 67\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "17.41993522644043 -> 61\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "73.64484405517578 -> 49\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "38.43193054199219 -> 44\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "22.056684494018555 -> 46\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "70.53095245361328 -> 34\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "45.65367126464844 -> 2\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "60.13587188720703 -> 57\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "38.10478973388672 -> 27\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "23.877227783203125 -> 35\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "38.71451187133789 -> 40\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "28.02663803100586 -> 54\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "40.69479751586914 -> 27\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "55.382240295410156 -> 11\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "30.431255340576172 -> 45\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "14.001663208007812 -> 58\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "69.574462890625 -> 2\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "70.97223663330078 -> 38\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "36.688995361328125 -> 71\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "23.620407104492188 -> 59\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "66.4746322631836 -> 69\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "87.54134368896484 -> 39\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "55.17674255371094 -> 79\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "41.649505615234375 -> 56\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "0.0 -> 48\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "60.24559020996094 -> 34\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "50.45110321044922 -> 34\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "47.273399353027344 -> 39\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "58.63101577758789 -> 59\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "44.71950149536133 -> 57\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "56.86791229248047 -> 53\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "77.2609634399414 -> 68\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "54.837684631347656 -> 51\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "48.679500579833984 -> 45\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "30.71796989440918 -> 17\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "61.686302185058594 -> 77\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "41.79139709472656 -> 59\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "49.45395278930664 -> 47\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "55.45372772216797 -> 71\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "54.3848762512207 -> 47\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "57.097740173339844 -> 58\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "66.57122802734375 -> 48\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "49.794349670410156 -> 64\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "70.21991729736328 -> 67\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "48.943336486816406 -> 52\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "17.043485641479492 -> 48\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "33.80133056640625 -> 43\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "34.49653625488281 -> 56\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "45.20249557495117 -> 65\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "65.05332946777344 -> 76\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "42.01374053955078 -> 18\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "66.27423095703125 -> 43\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "62.54658126831055 -> 32\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "84.7891616821289 -> 58\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "63.61905288696289 -> 72\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "57.21977615356445 -> 72\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "30.946533203125 -> 45\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "56.563682556152344 -> 59\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "35.73725509643555 -> 29\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "6.461964130401611 -> 55\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "58.840633392333984 -> 30\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "35.268226623535156 -> 23\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "69.59024047851562 -> 55\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "59.570289611816406 -> 66\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "51.75596237182617 -> 37\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "29.03057098388672 -> 73\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "50.802223205566406 -> 53\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "54.97261047363281 -> 65\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "65.23480224609375 -> 32\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "23.58660316467285 -> 73\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "59.569271087646484 -> 32\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "55.61603927612305 -> 75\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "36.09638595581055 -> 53\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "17.36455535888672 -> 55\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "47.90735626220703 -> 60\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "53.848506927490234 -> 42\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "35.40532302856445 -> 58\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "46.38507080078125 -> 17\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "62.94023895263672 -> 6\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "68.63944244384766 -> 43\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "8.962638854980469 -> 56\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "5.017361640930176 -> 65\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "66.2952880859375 -> 20\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "22.62851333618164 -> 58\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "69.74840545654297 -> 52\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "70.18441009521484 -> 55\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "80.33394622802734 -> 78\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "55.34511184692383 -> 17\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "67.15601348876953 -> 73\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "32.27298355102539 -> 52\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "54.826412200927734 -> 49\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "59.91150665283203 -> 34\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "72.04728698730469 -> 53\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "61.92064666748047 -> 23\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "60.78965377807617 -> 51\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "58.387725830078125 -> 49\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "53.77117156982422 -> 56\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "55.805118560791016 -> 60\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "53.72056198120117 -> 68\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "38.33769226074219 -> 21\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "45.023902893066406 -> 58\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "19.49698829650879 -> 56\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "45.55694580078125 -> 50\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "64.74754333496094 -> 8\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "66.06451416015625 -> 87\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "78.04973602294922 -> 40\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "64.42062377929688 -> 50\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "25.227636337280273 -> 7\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "66.29252624511719 -> 61\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "53.64051818847656 -> 31\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "53.68203353881836 -> 38\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "62.07185363769531 -> 51\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "55.587257385253906 -> 23\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "62.78154373168945 -> 36\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "69.2250747680664 -> 36\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "49.066261291503906 -> 59\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "43.45759582519531 -> 42\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "62.925628662109375 -> 56\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "33.33709716796875 -> 52\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "40.41606140136719 -> 46\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "52.23041534423828 -> 43\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "82.19950103759766 -> 62\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "44.15911102294922 -> 41\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "24.84972381591797 -> 49\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "26.736730575561523 -> 53\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "32.054019927978516 -> 27\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "74.13927459716797 -> 66\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "64.27548217773438 -> 53\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "29.717281341552734 -> 65\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "41.31429672241211 -> 66\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "24.74267578125 -> 77\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "66.26222229003906 -> 24\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "39.84901809692383 -> 56\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "63.527183532714844 -> 55\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "68.29922485351562 -> 0\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "86.35352325439453 -> 57\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "55.72380828857422 -> 76\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "57.46134948730469 -> 58\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "56.3584098815918 -> 25\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "35.33906936645508 -> 52\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "48.48044204711914 -> 58\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "41.754486083984375 -> 24\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "62.20913314819336 -> 57\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "0.0 -> 34\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "84.23954010009766 -> 72\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "53.245201110839844 -> 64\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "48.86207962036133 -> 60\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "76.76876068115234 -> 40\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "15.171091079711914 -> 68\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "50.098487854003906 -> 30\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "49.73564147949219 -> 44\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "39.917205810546875 -> 73\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "0.0 -> 8\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "43.424015045166016 -> 41\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "52.6275634765625 -> 44\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "34.0220832824707 -> 39\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "12.205940246582031 -> 63\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "20.55120277404785 -> 42\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "72.1333236694336 -> 56\n",
      "Mean Absolute Error: 19.91\n",
      "Accuracy :  59.13 %\n",
      "Accuracy of difference between +-15: 46.0\n"
     ]
    }
   ],
   "source": [
    "number_of_tests = 200\n",
    "sum_difference = 0\n",
    "predictions = []\n",
    "actual = y_test[:number_of_tests]\n",
    "accuracy_variance = 0 #number of values who are between +-10diff\n",
    "\n",
    "for i in range(number_of_tests):\n",
    "    predictions.append(model_regression_tuned.predict(X_test_scaled[i].reshape(1,12))[0][0])\n",
    "    print(f\"{predictions[i]} -> {y_test[i]}\")\n",
    "    sum_difference += abs(predictions[i] - y_test[i])\n",
    "    accuracy_variance += (np.abs(predictions[i]-y_test[i]) <= 15)\n",
    "    \n",
    "mape = 100 * (sum_difference / (np.sum(y_test[:number_of_tests])))\n",
    "accuracy = abs(100 - np.mean(mape))\n",
    "\n",
    "print(f\"Mean Absolute Error: {round((sum_difference/number_of_tests),2)}\")\n",
    "print(\"Accuracy : \", round(accuracy, 2), '%')\n",
    "print(f\"Accuracy of difference between +-15: {100*accuracy_variance/number_of_tests}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
